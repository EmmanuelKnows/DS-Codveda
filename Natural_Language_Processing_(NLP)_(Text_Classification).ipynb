{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRqF0tLu2uYB/7FfKCXSZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmanuelKnows/DS-Codveda/blob/main/Natural_Language_Processing_(NLP)_(Text_Classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP) - Text Classification"
      ],
      "metadata": {
        "id": "sSb-L7uLg89Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "Aa-ky5_Xg28h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxGuKV51g03Z",
        "outputId": "d1d9d558-6922-41d5-f177-3e54e91be7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Explore the Dataset"
      ],
      "metadata": {
        "id": "FfwFalB9hQIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Sentiment dataset.csv')\n",
        "\n",
        "# Display basic info\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "490iM_FfhMer",
        "outputId": "418110f5-541d-4a97-a2e3-c825e1ecfb8e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (732, 15)\n",
            "\n",
            "First few rows:\n",
            "   Unnamed: 0.1  Unnamed: 0  \\\n",
            "0             0           0   \n",
            "1             1           1   \n",
            "2             2           2   \n",
            "3             3           3   \n",
            "4             4           4   \n",
            "\n",
            "                                                Text    Sentiment  \\\n",
            "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
            "1   Traffic was terrible this morning.           ...   Negative     \n",
            "2   Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
            "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
            "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
            "\n",
            "             Timestamp            User     Platform  \\\n",
            "0  2023-01-15 12:30:00   User123          Twitter     \n",
            "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
            "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
            "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
            "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
            "\n",
            "                                     Hashtags  Retweets  Likes       Country  \\\n",
            "0   #Nature #Park                                  15.0   30.0     USA         \n",
            "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
            "2   #Fitness #Workout                              20.0   40.0   USA           \n",
            "3   #Travel #Adventure                              8.0   15.0     UK          \n",
            "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
            "\n",
            "   Year  Month  Day  Hour  \n",
            "0  2023      1   15    12  \n",
            "1  2023      1   15     8  \n",
            "2  2023      1   15    15  \n",
            "3  2023      1   15    18  \n",
            "4  2023      1   15    19  \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 732 entries, 0 to 731\n",
            "Data columns (total 15 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Unnamed: 0.1  732 non-null    int64  \n",
            " 1   Unnamed: 0    732 non-null    int64  \n",
            " 2   Text          732 non-null    object \n",
            " 3   Sentiment     732 non-null    object \n",
            " 4   Timestamp     732 non-null    object \n",
            " 5   User          732 non-null    object \n",
            " 6   Platform      732 non-null    object \n",
            " 7   Hashtags      732 non-null    object \n",
            " 8   Retweets      732 non-null    float64\n",
            " 9   Likes         732 non-null    float64\n",
            " 10  Country       732 non-null    object \n",
            " 11  Year          732 non-null    int64  \n",
            " 12  Month         732 non-null    int64  \n",
            " 13  Day           732 non-null    int64  \n",
            " 14  Hour          732 non-null    int64  \n",
            "dtypes: float64(2), int64(6), object(7)\n",
            "memory usage: 85.9+ KB\n",
            "None\n",
            "\n",
            "Missing Values:\n",
            "Unnamed: 0.1    0\n",
            "Unnamed: 0      0\n",
            "Text            0\n",
            "Sentiment       0\n",
            "Timestamp       0\n",
            "User            0\n",
            "Platform        0\n",
            "Hashtags        0\n",
            "Retweets        0\n",
            "Likes           0\n",
            "Country         0\n",
            "Year            0\n",
            "Month           0\n",
            "Day             0\n",
            "Hour            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique sentiments\n",
        "print(\"\\nUnique Sentiments:\")\n",
        "print(df['Sentiment'].unique())\n",
        "print(f\"\\nNumber of unique sentiments: {df['Sentiment'].nunique()}\")\n",
        "\n",
        "# Distribution of sentiments\n",
        "print(\"\\nSentiment Distribution:\")\n",
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "print(sentiment_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fXCJRqD8cQb",
        "outputId": "5bdc4e12-0cbd-4342-9224-c93f92aaca1b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique Sentiments:\n",
            "[' Positive  ' ' Negative  ' ' Neutral   ' ' Anger        '\n",
            " ' Fear         ' ' Sadness      ' ' Disgust      ' ' Happiness    '\n",
            " ' Joy          ' ' Love         ' ' Amusement    ' ' Enjoyment    '\n",
            " ' Admiration   ' ' Affection    ' ' Awe          ' ' Disappointed '\n",
            " ' Surprise     ' ' Acceptance   ' ' Adoration    ' ' Anticipation '\n",
            " ' Bitter       ' ' Calmness     ' ' Confusion    ' ' Excitement   '\n",
            " ' Kind         ' ' Pride        ' ' Shame        ' ' Confusion '\n",
            " ' Excitement ' ' Shame ' ' Elation       ' ' Euphoria      '\n",
            " ' Contentment   ' ' Serenity      ' ' Gratitude     ' ' Hope          '\n",
            " ' Empowerment   ' ' Compassion    ' ' Tenderness    ' ' Arousal       '\n",
            " ' Enthusiasm    ' ' Fulfillment  ' ' Reverence     ' ' Compassion'\n",
            " ' Fulfillment   ' ' Reverence ' ' Elation   ' ' Despair         '\n",
            " ' Grief           ' ' Loneliness      ' ' Jealousy        '\n",
            " ' Resentment      ' ' Frustration     ' ' Boredom         '\n",
            " ' Anxiety         ' ' Intimidation    ' ' Helplessness    '\n",
            " ' Envy            ' ' Regret         ' ' Disgust         '\n",
            " ' Despair      ' ' Loneliness ' ' Frustration ' ' Anxiety   '\n",
            " ' Intimidation ' ' Helplessness ' ' Jealousy    ' ' Curiosity       '\n",
            " ' Indifference    ' ' Confusion       ' ' Numbness        '\n",
            " ' Melancholy      ' ' Nostalgia       ' ' Ambivalence     '\n",
            " ' Acceptance      ' ' Determination   ' ' Serenity        ' ' Numbness '\n",
            " ' Zest ' ' Contentment ' ' Hopeful ' ' Proud ' ' Grateful '\n",
            " ' Empathetic ' ' Compassionate ' ' Playful ' ' Free-spirited '\n",
            " ' Inspired ' ' Confident ' ' Serenity ' ' Curiosity ' ' Ambivalence '\n",
            " ' Despair ' ' Bitterness ' ' Yearning ' ' Fearful ' ' Apprehensive '\n",
            " ' Overwhelmed ' ' Jealous ' ' Devastated ' ' Frustrated ' ' Envious '\n",
            " ' Dismissive ' ' Awe           ' ' Determination ' ' Nostalgia      '\n",
            " ' Thrill        ' ' Calmness      ' ' Overwhelmed   ' ' Gratitude   '\n",
            " ' Bittersweet ' ' Curiosity     ' ' Admiration    ' ' Overjoyed     '\n",
            " ' Inspiration   ' ' Motivation    ' ' Amusement     ' ' Contemplation '\n",
            " ' JoyfulReunion ' ' Excitement    ' ' Satisfaction  ' ' Blessed       '\n",
            " ' Anticipation  ' ' Reflection    ' ' Nostalgia     ' ' Appreciation  '\n",
            " ' Confidence    ' ' Surprise      ' ' Accomplishment ' ' Wonderment    '\n",
            " ' Optimism      ' ' Pride         ' ' Happiness     ' ' Curiosity   '\n",
            " ' Enchantment   ' ' Intrigue      ' ' PlayfulJoy    ' ' Mindfulness   '\n",
            " ' DreamChaser   ' ' Elegance ' ' Whimsy        ' ' Pensive '\n",
            " ' Thrill      ' ' Harmony    ' ' Creativity   ' ' Radiance    '\n",
            " ' Wonder     ' ' Rejuvenation ' ' Inspiration ' ' Coziness     '\n",
            " ' Gratitude  ' ' Adventure ' ' Euphoria   ' ' Awe    ' ' Melodic       '\n",
            " ' FestiveJoy    ' ' InnerJourney  ' ' Freedom       ' ' Dazzle        '\n",
            " ' Adrenaline     ' ' Harmony       ' ' ArtisticBurst ' ' Radiance      '\n",
            " ' Wonder       ' ' Inspiration  ' ' CulinaryOdyssey ' ' Euphoria     '\n",
            " ' Curiosity  ' ' Resilience   ' ' Immersion ' ' Nostalgia '\n",
            " ' Spark        ' ' Gratitude    ' ' Marvel       ' ' Serenity   '\n",
            " ' Heartbreak    ' ' Loneliness    ' ' Grief      ' ' Despair   '\n",
            " ' Betrayal      ' ' Suffering ' ' EmotionalStorm ' ' Regret        '\n",
            " ' Isolation ' ' Disappointment ' ' LostLove ' ' Melancholy '\n",
            " ' Exhaustion ' ' Sorrow      ' ' Darkness     ' ' Desperation '\n",
            " ' Ruins      ' ' Desolation ' ' Regret ' ' Grief ' ' Heartbreak '\n",
            " ' Betrayal ' ' Resilience ' ' Sorrow ' ' Loss ' ' Heartache '\n",
            " ' Solitude ' ' Joy ' ' Happiness ' ' Enthusiasm ' ' Gratitude '\n",
            " ' Positivity ' ' Kindness ' ' Friendship ' ' Love ' ' Surprise '\n",
            " ' Success ' ' Thrill ' ' Reflection ' ' Enchantment ' ' Exploration '\n",
            " ' Awe ' ' Amazement ' ' Romance ' ' Captivation ' ' Wonder '\n",
            " ' Tranquility ' ' Grandeur ' ' Emotion ' ' Energy ' ' Celebration '\n",
            " ' Charm ' ' Ecstasy ' ' Hope ' ' Creativity ' ' Colorful ' ' Pride '\n",
            " ' Hypnotic ' ' Connection ' ' Iconic ' ' Euphoria ' ' Journey '\n",
            " ' Engagement ' ' Touched ' ' Suspense ' ' Satisfaction ' ' Admiration '\n",
            " ' Triumph ' ' Heartwarming ' ' Obstacle ' ' Sympathy ' ' Pressure '\n",
            " ' Renewed Effort ' ' Miscalculation ' ' Challenge ' ' Solace '\n",
            " ' Breakthrough ' ' Harmony ' ' Joy in Baking ' ' Envisioning History '\n",
            " ' Imagination ' ' Vibrancy ' ' Mesmerizing ' ' Culinary Adventure '\n",
            " ' Winter Magic ' ' Thrilling Journey ' \" Nature's Beauty \"\n",
            " ' Celestial Wonder ' ' Creative Inspiration ' ' Runway Creativity '\n",
            " \" Ocean's Freedom \" ' Whispers of the Past ' ' Boredom ' ' Indifference '\n",
            " ' Disgust ' ' Relief ' ' Positive ' ' Embarrassed ' ' Mischievous '\n",
            " ' Sad ' ' Hate ' ' Bad ' ' Neutral ' ' Happy ']\n",
            "\n",
            "Number of unique sentiments: 279\n",
            "\n",
            "Sentiment Distribution:\n",
            "Sentiment\n",
            "Positive               44\n",
            "Joy                    42\n",
            "Excitement             32\n",
            "Happy                  14\n",
            "Neutral                14\n",
            "                       ..\n",
            "Vibrancy                1\n",
            "Culinary Adventure      1\n",
            "Mesmerizing             1\n",
            "Thrilling Journey       1\n",
            "Winter Magic            1\n",
            "Name: count, Length: 279, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Sentiment Column\n",
        "\n",
        "#df1 = df.copy()\n",
        "df['Sentiment'] = df['Sentiment'].str.strip()\n",
        "\n",
        "print(\"\\nUnique Sentiments:\")\n",
        "print(df['Sentiment'].value_counts())\n",
        "print(f\"\\nNumber of unique sentiments: {df['Sentiment'].nunique()}\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7mvbZERzuG0",
        "outputId": "f2674ef9-18b8-4d42-83a9-52319ecf6bcf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique Sentiments:\n",
            "Sentiment\n",
            "Positive                45\n",
            "Joy                     44\n",
            "Excitement              37\n",
            "Contentment             19\n",
            "Neutral                 18\n",
            "                        ..\n",
            "Celestial Wonder         1\n",
            "Nature's Beauty          1\n",
            "Thrilling Journey        1\n",
            "Whispers of the Past     1\n",
            "Relief                   1\n",
            "Name: count, Length: 191, dtype: int64\n",
            "\n",
            "Number of unique sentiments: 191\n",
            "   Unnamed: 0.1  Unnamed: 0  \\\n",
            "0             0           0   \n",
            "1             1           1   \n",
            "2             2           2   \n",
            "3             3           3   \n",
            "4             4           4   \n",
            "\n",
            "                                                Text Sentiment  \\\n",
            "0   Enjoying a beautiful day at the park!        ...  Positive   \n",
            "1   Traffic was terrible this morning.           ...  Negative   \n",
            "2   Just finished an amazing workout! ðŸ’ª          ...  Positive   \n",
            "3   Excited about the upcoming weekend getaway!  ...  Positive   \n",
            "4   Trying out a new recipe for dinner tonight.  ...   Neutral   \n",
            "\n",
            "             Timestamp            User     Platform  \\\n",
            "0  2023-01-15 12:30:00   User123          Twitter     \n",
            "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
            "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
            "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
            "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
            "\n",
            "                                     Hashtags  Retweets  Likes       Country  \\\n",
            "0   #Nature #Park                                  15.0   30.0     USA         \n",
            "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
            "2   #Fitness #Workout                              20.0   40.0   USA           \n",
            "3   #Travel #Adventure                              8.0   15.0     UK          \n",
            "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
            "\n",
            "   Year  Month  Day  Hour                    Processed_Text  \n",
            "0  2023      1   15    12       enjoying beautiful day park  \n",
            "1  2023      1   15     8          traffic terrible morning  \n",
            "2  2023      1   15    15          finished amazing workout  \n",
            "3  2023      1   15    18  excited upcoming weekend getaway  \n",
            "4  2023      1   15    19  trying new recipe dinner tonight  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(df['Sentiment'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3bJR4n49YFD",
        "outputId": "367515df-8fb6-4674-b214-eab9beb0cd66"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "dm8UeP0ah8gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Remove extra whitespaces\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove stopwords from tokens\"\"\"\n",
        "        return [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        \"\"\"Lemmatize tokens\"\"\"\n",
        "        return [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    def stem_tokens(self, tokens):\n",
        "        \"\"\"Stem tokens\"\"\"\n",
        "        return [self.stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    def preprocess_pipeline(self, text, use_stemming=False):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = self.tokenize_text(cleaned_text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "\n",
        "        # Apply stemming or lemmatization\n",
        "        if use_stemming:\n",
        "            tokens = self.stem_tokens(tokens)\n",
        "        else:\n",
        "            tokens = self.lemmatize_tokens(tokens)\n",
        "\n",
        "        # Join back to string\n",
        "        processed_text = ' '.join(tokens)\n",
        "\n",
        "        return processed_text\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Apply preprocessing to the Text column\n",
        "print(\"Processing text data...\")\n",
        "df['Processed_Text'] = df['Text'].apply(lambda x: preprocessor.preprocess_pipeline(x, use_stemming=False))\n",
        "\n",
        "# Show sample of processed text\n",
        "print(\"\\nOriginal vs Processed Text:\")\n",
        "sample_idx = np.random.randint(0, len(df))\n",
        "print(f\"Original: {df.loc[sample_idx, 'Text']}\")\n",
        "print(f\"Processed: {df.loc[sample_idx, 'Processed_Text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PffEOBCUhXJ3",
        "outputId": "65a14206-5b48-4b88-b1d2-62a9027e1a42"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text data...\n",
            "\n",
            "Original vs Processed Text:\n",
            "Original: Emotional exhaustion, the weight of the world crushing weary shoulders. \n",
            "Processed: emotional exhaustion weight world crushing weary shoulder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction using TF-IDF"
      ],
      "metadata": {
        "id": "NwSgbvezjIvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Use top 5000 features\n",
        "    min_df=5,          # Ignore terms with document frequency < 5\n",
        "    max_df=0.7,        # Ignore terms with document frequency > 70%\n",
        "    ngram_range=(1, 2) # Use unigrams and bigrams\n",
        ")\n",
        "\n",
        "# Fit and transform the processed text\n",
        "print(\"Creating TF-IDF features...\")\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['Processed_Text'])\n",
        "\n",
        "# Display feature matrix shape\n",
        "print(f\"TF-IDF Matrix Shape: {X_tfidf.shape}\")\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"Number of features: {len(feature_names)}\")\n",
        "print(f\"Sample features: {feature_names[:20]}\")\n",
        "\n",
        "# Get most important words for each sentiment (optional analysis)\n",
        "def get_top_tfidf_features(tfidf_matrix, feature_names, n=10):\n",
        "    \"\"\"Get top n features by TF-IDF score\"\"\"\n",
        "    sums = tfidf_matrix.sum(axis=0)\n",
        "    data = []\n",
        "    for col, term in enumerate(feature_names):\n",
        "        data.append((term, sums[0, col]))\n",
        "    ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
        "    return ranking.sort_values('rank', ascending=False).head(n)\n",
        "\n",
        "# Get top features overall\n",
        "top_features = get_top_tfidf_features(X_tfidf.sum(axis=0), feature_names, n=20)\n",
        "print(\"\\nTop 20 features by TF-IDF score:\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaCuZiqOjKnc",
        "outputId": "d469072f-7d20-4f76-9c39-8b3fd67ba98f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TF-IDF features...\n",
            "TF-IDF Matrix Shape: (732, 307)\n",
            "Number of features: 307\n",
            "Sample features: ['acceptance' 'accidentally' 'accomplishment' 'achievement' 'achieving'\n",
            " 'act' 'adventure' 'age' 'ahead' 'air' 'ambivalence' 'amidst' 'ancient'\n",
            " 'anticipation' 'anxiety' 'around' 'art' 'attended' 'attending' 'away']\n",
            "\n",
            "Top 20 features by TF-IDF score:\n",
            "          term       rank\n",
            "185        new  17.847044\n",
            "72         day  16.304499\n",
            "161       life  15.310601\n",
            "118     friend  13.360149\n",
            "106    feeling  12.381308\n",
            "186      night  11.158456\n",
            "178     moment  11.085681\n",
            "150        joy  10.811881\n",
            "43   challenge  10.648070\n",
            "162       like  10.531975\n",
            "79       dream  10.500017\n",
            "133      heart  10.293206\n",
            "155   laughter  10.042286\n",
            "305      world   9.903465\n",
            "212    project   9.312686\n",
            "81        echo   9.197734\n",
            "86     emotion   9.170434\n",
            "275       time   9.134892\n",
            "16         art   8.919873\n",
            "22      beauty   8.895100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Sentiment Labels"
      ],
      "metadata": {
        "id": "8ZhUidlqkPqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentiment labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['Sentiment'])\n",
        "\n",
        "# Check encoding mapping\n",
        "print(\"Label Encoding Mapping:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{i}: {label}\")\n",
        "\n",
        "# Store class names for later use\n",
        "class_names = label_encoder.classes_\n",
        "print(f\"\\nNumber of classes: {len(class_names)}\")\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nClass Distribution after encoding:\")\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"Class {label} ({class_names[label]}): {count} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13R_RuDHkPXj",
        "outputId": "252d15dc-fa49-4b9b-dbf7-0969164b727b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoding Mapping:\n",
            "0: Acceptance\n",
            "1: Accomplishment\n",
            "2: Admiration\n",
            "3: Adoration\n",
            "4: Adrenaline\n",
            "5: Adventure\n",
            "6: Affection\n",
            "7: Amazement\n",
            "8: Ambivalence\n",
            "9: Amusement\n",
            "10: Anger\n",
            "11: Anticipation\n",
            "12: Anxiety\n",
            "13: Appreciation\n",
            "14: Apprehensive\n",
            "15: Arousal\n",
            "16: ArtisticBurst\n",
            "17: Awe\n",
            "18: Bad\n",
            "19: Betrayal\n",
            "20: Bitter\n",
            "21: Bitterness\n",
            "22: Bittersweet\n",
            "23: Blessed\n",
            "24: Boredom\n",
            "25: Breakthrough\n",
            "26: Calmness\n",
            "27: Captivation\n",
            "28: Celebration\n",
            "29: Celestial Wonder\n",
            "30: Challenge\n",
            "31: Charm\n",
            "32: Colorful\n",
            "33: Compassion\n",
            "34: Compassionate\n",
            "35: Confidence\n",
            "36: Confident\n",
            "37: Confusion\n",
            "38: Connection\n",
            "39: Contemplation\n",
            "40: Contentment\n",
            "41: Coziness\n",
            "42: Creative Inspiration\n",
            "43: Creativity\n",
            "44: Culinary Adventure\n",
            "45: CulinaryOdyssey\n",
            "46: Curiosity\n",
            "47: Darkness\n",
            "48: Dazzle\n",
            "49: Desolation\n",
            "50: Despair\n",
            "51: Desperation\n",
            "52: Determination\n",
            "53: Devastated\n",
            "54: Disappointed\n",
            "55: Disappointment\n",
            "56: Disgust\n",
            "57: Dismissive\n",
            "58: DreamChaser\n",
            "59: Ecstasy\n",
            "60: Elation\n",
            "61: Elegance\n",
            "62: Embarrassed\n",
            "63: Emotion\n",
            "64: EmotionalStorm\n",
            "65: Empathetic\n",
            "66: Empowerment\n",
            "67: Enchantment\n",
            "68: Energy\n",
            "69: Engagement\n",
            "70: Enjoyment\n",
            "71: Enthusiasm\n",
            "72: Envious\n",
            "73: Envisioning History\n",
            "74: Envy\n",
            "75: Euphoria\n",
            "76: Excitement\n",
            "77: Exhaustion\n",
            "78: Exploration\n",
            "79: Fear\n",
            "80: Fearful\n",
            "81: FestiveJoy\n",
            "82: Free-spirited\n",
            "83: Freedom\n",
            "84: Friendship\n",
            "85: Frustrated\n",
            "86: Frustration\n",
            "87: Fulfillment\n",
            "88: Grandeur\n",
            "89: Grateful\n",
            "90: Gratitude\n",
            "91: Grief\n",
            "92: Happiness\n",
            "93: Happy\n",
            "94: Harmony\n",
            "95: Hate\n",
            "96: Heartache\n",
            "97: Heartbreak\n",
            "98: Heartwarming\n",
            "99: Helplessness\n",
            "100: Hope\n",
            "101: Hopeful\n",
            "102: Hypnotic\n",
            "103: Iconic\n",
            "104: Imagination\n",
            "105: Immersion\n",
            "106: Indifference\n",
            "107: InnerJourney\n",
            "108: Inspiration\n",
            "109: Inspired\n",
            "110: Intimidation\n",
            "111: Intrigue\n",
            "112: Isolation\n",
            "113: Jealous\n",
            "114: Jealousy\n",
            "115: Journey\n",
            "116: Joy\n",
            "117: Joy in Baking\n",
            "118: JoyfulReunion\n",
            "119: Kind\n",
            "120: Kindness\n",
            "121: Loneliness\n",
            "122: Loss\n",
            "123: LostLove\n",
            "124: Love\n",
            "125: Marvel\n",
            "126: Melancholy\n",
            "127: Melodic\n",
            "128: Mesmerizing\n",
            "129: Mindfulness\n",
            "130: Miscalculation\n",
            "131: Mischievous\n",
            "132: Motivation\n",
            "133: Nature's Beauty\n",
            "134: Negative\n",
            "135: Neutral\n",
            "136: Nostalgia\n",
            "137: Numbness\n",
            "138: Obstacle\n",
            "139: Ocean's Freedom\n",
            "140: Optimism\n",
            "141: Overjoyed\n",
            "142: Overwhelmed\n",
            "143: Pensive\n",
            "144: Playful\n",
            "145: PlayfulJoy\n",
            "146: Positive\n",
            "147: Positivity\n",
            "148: Pressure\n",
            "149: Pride\n",
            "150: Proud\n",
            "151: Radiance\n",
            "152: Reflection\n",
            "153: Regret\n",
            "154: Rejuvenation\n",
            "155: Relief\n",
            "156: Renewed Effort\n",
            "157: Resentment\n",
            "158: Resilience\n",
            "159: Reverence\n",
            "160: Romance\n",
            "161: Ruins\n",
            "162: Runway Creativity\n",
            "163: Sad\n",
            "164: Sadness\n",
            "165: Satisfaction\n",
            "166: Serenity\n",
            "167: Shame\n",
            "168: Solace\n",
            "169: Solitude\n",
            "170: Sorrow\n",
            "171: Spark\n",
            "172: Success\n",
            "173: Suffering\n",
            "174: Surprise\n",
            "175: Suspense\n",
            "176: Sympathy\n",
            "177: Tenderness\n",
            "178: Thrill\n",
            "179: Thrilling Journey\n",
            "180: Touched\n",
            "181: Tranquility\n",
            "182: Triumph\n",
            "183: Vibrancy\n",
            "184: Whimsy\n",
            "185: Whispers of the Past\n",
            "186: Winter Magic\n",
            "187: Wonder\n",
            "188: Wonderment\n",
            "189: Yearning\n",
            "190: Zest\n",
            "\n",
            "Number of classes: 191\n",
            "\n",
            "Class Distribution after encoding:\n",
            "Class 0 (Acceptance): 8 samples\n",
            "Class 1 (Accomplishment): 3 samples\n",
            "Class 2 (Admiration): 4 samples\n",
            "Class 3 (Adoration): 2 samples\n",
            "Class 4 (Adrenaline): 1 samples\n",
            "Class 5 (Adventure): 3 samples\n",
            "Class 6 (Affection): 2 samples\n",
            "Class 7 (Amazement): 1 samples\n",
            "Class 8 (Ambivalence): 6 samples\n",
            "Class 9 (Amusement): 3 samples\n",
            "Class 10 (Anger): 2 samples\n",
            "Class 11 (Anticipation): 3 samples\n",
            "Class 12 (Anxiety): 2 samples\n",
            "Class 13 (Appreciation): 1 samples\n",
            "Class 14 (Apprehensive): 2 samples\n",
            "Class 15 (Arousal): 4 samples\n",
            "Class 16 (ArtisticBurst): 1 samples\n",
            "Class 17 (Awe): 9 samples\n",
            "Class 18 (Bad): 6 samples\n",
            "Class 19 (Betrayal): 5 samples\n",
            "Class 20 (Bitter): 3 samples\n",
            "Class 21 (Bitterness): 5 samples\n",
            "Class 22 (Bittersweet): 1 samples\n",
            "Class 23 (Blessed): 1 samples\n",
            "Class 24 (Boredom): 4 samples\n",
            "Class 25 (Breakthrough): 1 samples\n",
            "Class 26 (Calmness): 4 samples\n",
            "Class 27 (Captivation): 2 samples\n",
            "Class 28 (Celebration): 1 samples\n",
            "Class 29 (Celestial Wonder): 1 samples\n",
            "Class 30 (Challenge): 1 samples\n",
            "Class 31 (Charm): 1 samples\n",
            "Class 32 (Colorful): 1 samples\n",
            "Class 33 (Compassion): 4 samples\n",
            "Class 34 (Compassionate): 4 samples\n",
            "Class 35 (Confidence): 1 samples\n",
            "Class 36 (Confident): 3 samples\n",
            "Class 37 (Confusion): 8 samples\n",
            "Class 38 (Connection): 1 samples\n",
            "Class 39 (Contemplation): 2 samples\n",
            "Class 40 (Contentment): 19 samples\n",
            "Class 41 (Coziness): 2 samples\n",
            "Class 42 (Creative Inspiration): 1 samples\n",
            "Class 43 (Creativity): 3 samples\n",
            "Class 44 (Culinary Adventure): 1 samples\n",
            "Class 45 (CulinaryOdyssey): 1 samples\n",
            "Class 46 (Curiosity): 16 samples\n",
            "Class 47 (Darkness): 1 samples\n",
            "Class 48 (Dazzle): 1 samples\n",
            "Class 49 (Desolation): 4 samples\n",
            "Class 50 (Despair): 11 samples\n",
            "Class 51 (Desperation): 1 samples\n",
            "Class 52 (Determination): 7 samples\n",
            "Class 53 (Devastated): 3 samples\n",
            "Class 54 (Disappointed): 2 samples\n",
            "Class 55 (Disappointment): 2 samples\n",
            "Class 56 (Disgust): 5 samples\n",
            "Class 57 (Dismissive): 3 samples\n",
            "Class 58 (DreamChaser): 1 samples\n",
            "Class 59 (Ecstasy): 1 samples\n",
            "Class 60 (Elation): 7 samples\n",
            "Class 61 (Elegance): 1 samples\n",
            "Class 62 (Embarrassed): 8 samples\n",
            "Class 63 (Emotion): 2 samples\n",
            "Class 64 (EmotionalStorm): 1 samples\n",
            "Class 65 (Empathetic): 3 samples\n",
            "Class 66 (Empowerment): 5 samples\n",
            "Class 67 (Enchantment): 4 samples\n",
            "Class 68 (Energy): 1 samples\n",
            "Class 69 (Engagement): 1 samples\n",
            "Class 70 (Enjoyment): 2 samples\n",
            "Class 71 (Enthusiasm): 7 samples\n",
            "Class 72 (Envious): 3 samples\n",
            "Class 73 (Envisioning History): 1 samples\n",
            "Class 74 (Envy): 2 samples\n",
            "Class 75 (Euphoria): 7 samples\n",
            "Class 76 (Excitement): 37 samples\n",
            "Class 77 (Exhaustion): 1 samples\n",
            "Class 78 (Exploration): 2 samples\n",
            "Class 79 (Fear): 2 samples\n",
            "Class 80 (Fearful): 3 samples\n",
            "Class 81 (FestiveJoy): 1 samples\n",
            "Class 82 (Free-spirited): 3 samples\n",
            "Class 83 (Freedom): 1 samples\n",
            "Class 84 (Friendship): 1 samples\n",
            "Class 85 (Frustrated): 5 samples\n",
            "Class 86 (Frustration): 6 samples\n",
            "Class 87 (Fulfillment): 4 samples\n",
            "Class 88 (Grandeur): 1 samples\n",
            "Class 89 (Grateful): 4 samples\n",
            "Class 90 (Gratitude): 18 samples\n",
            "Class 91 (Grief): 9 samples\n",
            "Class 92 (Happiness): 5 samples\n",
            "Class 93 (Happy): 14 samples\n",
            "Class 94 (Harmony): 3 samples\n",
            "Class 95 (Hate): 6 samples\n",
            "Class 96 (Heartache): 1 samples\n",
            "Class 97 (Heartbreak): 3 samples\n",
            "Class 98 (Heartwarming): 1 samples\n",
            "Class 99 (Helplessness): 2 samples\n",
            "Class 100 (Hope): 5 samples\n",
            "Class 101 (Hopeful): 9 samples\n",
            "Class 102 (Hypnotic): 1 samples\n",
            "Class 103 (Iconic): 1 samples\n",
            "Class 104 (Imagination): 1 samples\n",
            "Class 105 (Immersion): 1 samples\n",
            "Class 106 (Indifference): 6 samples\n",
            "Class 107 (InnerJourney): 1 samples\n",
            "Class 108 (Inspiration): 6 samples\n",
            "Class 109 (Inspired): 5 samples\n",
            "Class 110 (Intimidation): 2 samples\n",
            "Class 111 (Intrigue): 1 samples\n",
            "Class 112 (Isolation): 2 samples\n",
            "Class 113 (Jealous): 3 samples\n",
            "Class 114 (Jealousy): 3 samples\n",
            "Class 115 (Journey): 1 samples\n",
            "Class 116 (Joy): 44 samples\n",
            "Class 117 (Joy in Baking): 1 samples\n",
            "Class 118 (JoyfulReunion): 1 samples\n",
            "Class 119 (Kind): 3 samples\n",
            "Class 120 (Kindness): 1 samples\n",
            "Class 121 (Loneliness): 9 samples\n",
            "Class 122 (Loss): 2 samples\n",
            "Class 123 (LostLove): 1 samples\n",
            "Class 124 (Love): 3 samples\n",
            "Class 125 (Marvel): 1 samples\n",
            "Class 126 (Melancholy): 6 samples\n",
            "Class 127 (Melodic): 1 samples\n",
            "Class 128 (Mesmerizing): 1 samples\n",
            "Class 129 (Mindfulness): 1 samples\n",
            "Class 130 (Miscalculation): 1 samples\n",
            "Class 131 (Mischievous): 2 samples\n",
            "Class 132 (Motivation): 1 samples\n",
            "Class 133 (Nature's Beauty): 1 samples\n",
            "Class 134 (Negative): 4 samples\n",
            "Class 135 (Neutral): 18 samples\n",
            "Class 136 (Nostalgia): 11 samples\n",
            "Class 137 (Numbness): 6 samples\n",
            "Class 138 (Obstacle): 1 samples\n",
            "Class 139 (Ocean's Freedom): 1 samples\n",
            "Class 140 (Optimism): 1 samples\n",
            "Class 141 (Overjoyed): 1 samples\n",
            "Class 142 (Overwhelmed): 4 samples\n",
            "Class 143 (Pensive): 1 samples\n",
            "Class 144 (Playful): 6 samples\n",
            "Class 145 (PlayfulJoy): 1 samples\n",
            "Class 146 (Positive): 45 samples\n",
            "Class 147 (Positivity): 1 samples\n",
            "Class 148 (Pressure): 1 samples\n",
            "Class 149 (Pride): 7 samples\n",
            "Class 150 (Proud): 4 samples\n",
            "Class 151 (Radiance): 2 samples\n",
            "Class 152 (Reflection): 4 samples\n",
            "Class 153 (Regret): 6 samples\n",
            "Class 154 (Rejuvenation): 2 samples\n",
            "Class 155 (Relief): 1 samples\n",
            "Class 156 (Renewed Effort): 1 samples\n",
            "Class 157 (Resentment): 3 samples\n",
            "Class 158 (Resilience): 2 samples\n",
            "Class 159 (Reverence): 4 samples\n",
            "Class 160 (Romance): 1 samples\n",
            "Class 161 (Ruins): 1 samples\n",
            "Class 162 (Runway Creativity): 1 samples\n",
            "Class 163 (Sad): 9 samples\n",
            "Class 164 (Sadness): 2 samples\n",
            "Class 165 (Satisfaction): 3 samples\n",
            "Class 166 (Serenity): 15 samples\n",
            "Class 167 (Shame): 3 samples\n",
            "Class 168 (Solace): 1 samples\n",
            "Class 169 (Solitude): 1 samples\n",
            "Class 170 (Sorrow): 2 samples\n",
            "Class 171 (Spark): 1 samples\n",
            "Class 172 (Success): 1 samples\n",
            "Class 173 (Suffering): 1 samples\n",
            "Class 174 (Surprise): 6 samples\n",
            "Class 175 (Suspense): 1 samples\n",
            "Class 176 (Sympathy): 1 samples\n",
            "Class 177 (Tenderness): 4 samples\n",
            "Class 178 (Thrill): 4 samples\n",
            "Class 179 (Thrilling Journey): 1 samples\n",
            "Class 180 (Touched): 1 samples\n",
            "Class 181 (Tranquility): 2 samples\n",
            "Class 182 (Triumph): 1 samples\n",
            "Class 183 (Vibrancy): 1 samples\n",
            "Class 184 (Whimsy): 2 samples\n",
            "Class 185 (Whispers of the Past): 1 samples\n",
            "Class 186 (Winter Magic): 1 samples\n",
            "Class 187 (Wonder): 3 samples\n",
            "Class 188 (Wonderment): 1 samples\n",
            "Class 189 (Yearning): 2 samples\n",
            "Class 190 (Zest): 2 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data into Train and Test Sets"
      ],
      "metadata": {
        "id": "2OSs1_zEle5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify classes with only one sample to exclude them from stratified split\n",
        "unique_classes, counts_classes = np.unique(y, return_counts=True)\n",
        "classes_to_keep = unique_classes[counts_classes >= 2]\n",
        "\n",
        "# Create a boolean mask for samples belonging to classes with at least 2 instances\n",
        "mask = np.isin(y, classes_to_keep)\n",
        "\n",
        "# Filter X_tfidf and y\n",
        "X_filtered = X_tfidf[mask]\n",
        "y_filtered = y[mask]\n",
        "\n",
        "print(f\"Original dataset size: {len(y)}\")\n",
        "print(f\"Dataset size after filtering single-sample classes: {len(y_filtered)}\")\n",
        "\n",
        "# Split data into train and test sets using the filtered data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_filtered, y_filtered, test_size=0.25, random_state=42, stratify=y_filtered\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "# Check class distribution in splits\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "for label, count in zip(unique_train, counts_train):\n",
        "    print(f\"Class {label} ({class_names[label]}): {count} samples\")\n",
        "\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "for label, count in zip(unique_test, counts_test):\n",
        "    print(f\"Class {label} ({class_names[label]}): {count} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5V7FMaDkWWk",
        "outputId": "fc82b4c5-d3c0-47cb-ea8b-7183990e3365"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 732\n",
            "Dataset size after filtering single-sample classes: 653\n",
            "\n",
            "Training set size: 489\n",
            "Test set size: 164\n",
            "Number of features: 307\n",
            "\n",
            "Class distribution in training set:\n",
            "Class 0 (Acceptance): 6 samples\n",
            "Class 1 (Accomplishment): 2 samples\n",
            "Class 2 (Admiration): 3 samples\n",
            "Class 3 (Adoration): 2 samples\n",
            "Class 5 (Adventure): 2 samples\n",
            "Class 6 (Affection): 2 samples\n",
            "Class 8 (Ambivalence): 4 samples\n",
            "Class 9 (Amusement): 2 samples\n",
            "Class 10 (Anger): 2 samples\n",
            "Class 11 (Anticipation): 2 samples\n",
            "Class 12 (Anxiety): 2 samples\n",
            "Class 14 (Apprehensive): 2 samples\n",
            "Class 15 (Arousal): 3 samples\n",
            "Class 17 (Awe): 7 samples\n",
            "Class 18 (Bad): 4 samples\n",
            "Class 19 (Betrayal): 4 samples\n",
            "Class 20 (Bitter): 2 samples\n",
            "Class 21 (Bitterness): 4 samples\n",
            "Class 24 (Boredom): 3 samples\n",
            "Class 26 (Calmness): 3 samples\n",
            "Class 27 (Captivation): 2 samples\n",
            "Class 33 (Compassion): 3 samples\n",
            "Class 34 (Compassionate): 3 samples\n",
            "Class 36 (Confident): 2 samples\n",
            "Class 37 (Confusion): 6 samples\n",
            "Class 39 (Contemplation): 2 samples\n",
            "Class 40 (Contentment): 14 samples\n",
            "Class 41 (Coziness): 2 samples\n",
            "Class 43 (Creativity): 2 samples\n",
            "Class 46 (Curiosity): 12 samples\n",
            "Class 49 (Desolation): 3 samples\n",
            "Class 50 (Despair): 8 samples\n",
            "Class 52 (Determination): 5 samples\n",
            "Class 53 (Devastated): 2 samples\n",
            "Class 54 (Disappointed): 2 samples\n",
            "Class 55 (Disappointment): 1 samples\n",
            "Class 56 (Disgust): 4 samples\n",
            "Class 57 (Dismissive): 2 samples\n",
            "Class 60 (Elation): 5 samples\n",
            "Class 62 (Embarrassed): 6 samples\n",
            "Class 63 (Emotion): 2 samples\n",
            "Class 65 (Empathetic): 2 samples\n",
            "Class 66 (Empowerment): 4 samples\n",
            "Class 67 (Enchantment): 3 samples\n",
            "Class 70 (Enjoyment): 1 samples\n",
            "Class 71 (Enthusiasm): 5 samples\n",
            "Class 72 (Envious): 2 samples\n",
            "Class 74 (Envy): 2 samples\n",
            "Class 75 (Euphoria): 5 samples\n",
            "Class 76 (Excitement): 28 samples\n",
            "Class 78 (Exploration): 2 samples\n",
            "Class 79 (Fear): 2 samples\n",
            "Class 80 (Fearful): 2 samples\n",
            "Class 82 (Free-spirited): 2 samples\n",
            "Class 85 (Frustrated): 4 samples\n",
            "Class 86 (Frustration): 4 samples\n",
            "Class 87 (Fulfillment): 3 samples\n",
            "Class 89 (Grateful): 3 samples\n",
            "Class 90 (Gratitude): 13 samples\n",
            "Class 91 (Grief): 7 samples\n",
            "Class 92 (Happiness): 4 samples\n",
            "Class 93 (Happy): 10 samples\n",
            "Class 94 (Harmony): 2 samples\n",
            "Class 95 (Hate): 4 samples\n",
            "Class 97 (Heartbreak): 2 samples\n",
            "Class 99 (Helplessness): 2 samples\n",
            "Class 100 (Hope): 4 samples\n",
            "Class 101 (Hopeful): 7 samples\n",
            "Class 106 (Indifference): 4 samples\n",
            "Class 108 (Inspiration): 4 samples\n",
            "Class 109 (Inspired): 4 samples\n",
            "Class 110 (Intimidation): 1 samples\n",
            "Class 112 (Isolation): 2 samples\n",
            "Class 113 (Jealous): 2 samples\n",
            "Class 114 (Jealousy): 2 samples\n",
            "Class 116 (Joy): 33 samples\n",
            "Class 119 (Kind): 2 samples\n",
            "Class 121 (Loneliness): 7 samples\n",
            "Class 122 (Loss): 2 samples\n",
            "Class 124 (Love): 2 samples\n",
            "Class 126 (Melancholy): 4 samples\n",
            "Class 131 (Mischievous): 2 samples\n",
            "Class 134 (Negative): 3 samples\n",
            "Class 135 (Neutral): 13 samples\n",
            "Class 136 (Nostalgia): 8 samples\n",
            "Class 137 (Numbness): 4 samples\n",
            "Class 142 (Overwhelmed): 3 samples\n",
            "Class 144 (Playful): 4 samples\n",
            "Class 146 (Positive): 34 samples\n",
            "Class 149 (Pride): 5 samples\n",
            "Class 150 (Proud): 3 samples\n",
            "Class 151 (Radiance): 2 samples\n",
            "Class 152 (Reflection): 3 samples\n",
            "Class 153 (Regret): 4 samples\n",
            "Class 154 (Rejuvenation): 2 samples\n",
            "Class 157 (Resentment): 2 samples\n",
            "Class 158 (Resilience): 2 samples\n",
            "Class 159 (Reverence): 3 samples\n",
            "Class 163 (Sad): 7 samples\n",
            "Class 164 (Sadness): 2 samples\n",
            "Class 165 (Satisfaction): 2 samples\n",
            "Class 166 (Serenity): 11 samples\n",
            "Class 167 (Shame): 2 samples\n",
            "Class 170 (Sorrow): 1 samples\n",
            "Class 174 (Surprise): 4 samples\n",
            "Class 177 (Tenderness): 3 samples\n",
            "Class 178 (Thrill): 3 samples\n",
            "Class 181 (Tranquility): 2 samples\n",
            "Class 184 (Whimsy): 2 samples\n",
            "Class 187 (Wonder): 2 samples\n",
            "Class 189 (Yearning): 2 samples\n",
            "Class 190 (Zest): 2 samples\n",
            "\n",
            "Class distribution in test set:\n",
            "Class 0 (Acceptance): 2 samples\n",
            "Class 1 (Accomplishment): 1 samples\n",
            "Class 2 (Admiration): 1 samples\n",
            "Class 5 (Adventure): 1 samples\n",
            "Class 8 (Ambivalence): 2 samples\n",
            "Class 9 (Amusement): 1 samples\n",
            "Class 11 (Anticipation): 1 samples\n",
            "Class 15 (Arousal): 1 samples\n",
            "Class 17 (Awe): 2 samples\n",
            "Class 18 (Bad): 2 samples\n",
            "Class 19 (Betrayal): 1 samples\n",
            "Class 20 (Bitter): 1 samples\n",
            "Class 21 (Bitterness): 1 samples\n",
            "Class 24 (Boredom): 1 samples\n",
            "Class 26 (Calmness): 1 samples\n",
            "Class 33 (Compassion): 1 samples\n",
            "Class 34 (Compassionate): 1 samples\n",
            "Class 36 (Confident): 1 samples\n",
            "Class 37 (Confusion): 2 samples\n",
            "Class 40 (Contentment): 5 samples\n",
            "Class 43 (Creativity): 1 samples\n",
            "Class 46 (Curiosity): 4 samples\n",
            "Class 49 (Desolation): 1 samples\n",
            "Class 50 (Despair): 3 samples\n",
            "Class 52 (Determination): 2 samples\n",
            "Class 53 (Devastated): 1 samples\n",
            "Class 55 (Disappointment): 1 samples\n",
            "Class 56 (Disgust): 1 samples\n",
            "Class 57 (Dismissive): 1 samples\n",
            "Class 60 (Elation): 2 samples\n",
            "Class 62 (Embarrassed): 2 samples\n",
            "Class 65 (Empathetic): 1 samples\n",
            "Class 66 (Empowerment): 1 samples\n",
            "Class 67 (Enchantment): 1 samples\n",
            "Class 70 (Enjoyment): 1 samples\n",
            "Class 71 (Enthusiasm): 2 samples\n",
            "Class 72 (Envious): 1 samples\n",
            "Class 75 (Euphoria): 2 samples\n",
            "Class 76 (Excitement): 9 samples\n",
            "Class 80 (Fearful): 1 samples\n",
            "Class 82 (Free-spirited): 1 samples\n",
            "Class 85 (Frustrated): 1 samples\n",
            "Class 86 (Frustration): 2 samples\n",
            "Class 87 (Fulfillment): 1 samples\n",
            "Class 89 (Grateful): 1 samples\n",
            "Class 90 (Gratitude): 5 samples\n",
            "Class 91 (Grief): 2 samples\n",
            "Class 92 (Happiness): 1 samples\n",
            "Class 93 (Happy): 4 samples\n",
            "Class 94 (Harmony): 1 samples\n",
            "Class 95 (Hate): 2 samples\n",
            "Class 97 (Heartbreak): 1 samples\n",
            "Class 100 (Hope): 1 samples\n",
            "Class 101 (Hopeful): 2 samples\n",
            "Class 106 (Indifference): 2 samples\n",
            "Class 108 (Inspiration): 2 samples\n",
            "Class 109 (Inspired): 1 samples\n",
            "Class 110 (Intimidation): 1 samples\n",
            "Class 113 (Jealous): 1 samples\n",
            "Class 114 (Jealousy): 1 samples\n",
            "Class 116 (Joy): 11 samples\n",
            "Class 119 (Kind): 1 samples\n",
            "Class 121 (Loneliness): 2 samples\n",
            "Class 124 (Love): 1 samples\n",
            "Class 126 (Melancholy): 2 samples\n",
            "Class 134 (Negative): 1 samples\n",
            "Class 135 (Neutral): 5 samples\n",
            "Class 136 (Nostalgia): 3 samples\n",
            "Class 137 (Numbness): 2 samples\n",
            "Class 142 (Overwhelmed): 1 samples\n",
            "Class 144 (Playful): 2 samples\n",
            "Class 146 (Positive): 11 samples\n",
            "Class 149 (Pride): 2 samples\n",
            "Class 150 (Proud): 1 samples\n",
            "Class 152 (Reflection): 1 samples\n",
            "Class 153 (Regret): 2 samples\n",
            "Class 157 (Resentment): 1 samples\n",
            "Class 159 (Reverence): 1 samples\n",
            "Class 163 (Sad): 2 samples\n",
            "Class 165 (Satisfaction): 1 samples\n",
            "Class 166 (Serenity): 4 samples\n",
            "Class 167 (Shame): 1 samples\n",
            "Class 170 (Sorrow): 1 samples\n",
            "Class 174 (Surprise): 2 samples\n",
            "Class 177 (Tenderness): 1 samples\n",
            "Class 178 (Thrill): 1 samples\n",
            "Class 187 (Wonder): 1 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Classification Models And Evaluation"
      ],
      "metadata": {
        "id": "wZICOBwCqYpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
        "    \"\"\"Train a model and evaluate its performance\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Get actual labels present in y_test and y_pred\n",
        "    actual_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
        "    # Filter class names to match the actual labels\n",
        "    filtered_class_names = [class_names[label] for label in actual_labels]\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_test, y_pred, target_names=filtered_class_names, output_dict=True, labels=actual_labels)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=actual_labels)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'report': report,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(alpha=0.1),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'SVM': SVC(kernel='linear', random_state=42, probability=True),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    results[model_name] = train_and_evaluate_model(\n",
        "        model, X_train, y_train, X_test, y_test, model_name\n",
        "    )\n",
        "\n",
        "    print(f\"Accuracy: {results[model_name]['accuracy']:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRJi-guGljFT",
        "outputId": "a50468e8-f78e-4109-fb44-491f15a20a88"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training Naive Bayes...\n",
            "Accuracy: 0.4451\n",
            "\n",
            "============================================================\n",
            "Training Logistic Regression...\n",
            "Accuracy: 0.4024\n",
            "\n",
            "============================================================\n",
            "Training SVM...\n",
            "Accuracy: 0.4817\n",
            "\n",
            "============================================================\n",
            "Training Random Forest...\n",
            "Accuracy: 0.5183\n"
          ]
        }
      ]
    }
  ]
}